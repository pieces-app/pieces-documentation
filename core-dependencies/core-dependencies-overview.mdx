---
sidebarTitle: "Core Dependencies"
---

---

<img
  className="mx-auto"
/>

---

## What Are Core Dependencies?

Pieces for Developers products, including the [Pieces for Developers Desktop Application](https://docs.pieces.app/products/desktop), utilize _two core dependencies_ to provide a local, secure, and efficient development experience—[PiecesOS](https://docs.pieces.app/products/core-dependencies/pieces-os) and [Ollama](https://docs.pieces.app/products/core-dependencies/ollama)**.**

## What Are They?

To run any Pieces software, you will need **[1] PiecesOS,** the backbone of the Pieces Suite. This lightweight application runs in the background of your device.

It powers the [Long-Term Memory (LTM-2) Engine](https://docs.pieces.app/products/core-dependencies/pieces-os#ltm-2), [Pieces Drive,](https://docs.pieces.app/products/desktop/drive) and the [Pieces Copilot.](https://docs.pieces.app/products/desktop/copilot)

Running local LLMs requires downloading and installing the **[2] Ollama** wrapper to power on-device AI capabilities, such as querying Pieces Copilot or the local inference required by the LTM-2 Engine.

1. **PiecesOS**: The backbone of the Pieces suite, managing local memory, AI-driven workflow enhancements, [Pieces MCP](https://docs.pieces.app/products/mcp/get-started), and other integrations within your development environment.
2. **Ollama**: A specialized wrapper that enables local AI inference, allowing Pieces Copilot and other features to leverage machine learning models _directly on your device._

## What Do They Do?

These dependencies—**PiecesOS and Ollama**—are lightweight services and engines that handle everything from local model management and context storage to advanced local inference for AI-assisted workflows.

<img
  className="mx-auto"
/>

PiecesOS is **required** for all Pieces products, including:

- Pieces for Developers Desktop App
- Plugins & Extensions for [JetBrains](https://docs.pieces.app/products/extensions-plugins/jetbrains), [VS Code](https://docs.pieces.app/products/extensions-plugins/visual-studio-code), [Sublime Text](https://docs.pieces.app/products/extensions-plugins/sublime), [JupyterLab](https://docs.pieces.app/products/extensions-plugins/jupyterlab), [Azure Data Studio](https://docs.pieces.app/products/extensions-plugins/azure-data-studio), [Neovim](https://docs.pieces.app/products/extensions-plugins/neovim-plugin), [Raycast](https://docs.pieces.app/products/raycast), [Obsidian](https://docs.pieces.app/products/obsidian), [the Pieces CLI](https://docs.pieces.app/products/extensions-plugins/cli), and more.

## Why Do We Need Them?

Pieces for Developers is designed with **speed and efficiency** in mind, so PiecesOS acts as the end-all between different Pieces products to minimize client-side overhead and additional code while also being secure and highly configurable.

<img
  className="mx-auto"
/>

Our focus on **security and flexibility** is why we’ve introduced the Ollama wrapper for local large language models—users can switch entirely to on-device generative AI, and by offloading most operations locally, the user experience benefits from:

- **Instant AI-powered assistance** without cloud latency.
- **100% local memory storage** with full control over data.
- **Offline functionality**, ensuring a seamless experience even when disconnected from the internet.
- **Lightweight, background operation**, consuming minimal system resources.

However, you don't have to install Ollama if you don't want to use it.

You can install it if you want to use local models, which is especially useful in enterprise settings where strong device security is important.

---

| **Dependency** | **Purpose**                                                           | **Required?**                                     |
| -------------- | --------------------------------------------------------------------- | ------------------------------------------------- |
| _PiecesOS_     | Manages memory, developer material storage, and plugin communication. | Yes — this is required for all Pieces products.   |
| _Ollama_       | Enables locally powered generative AI queries and model execution.    | No — but this is required for local AI inference. |

---
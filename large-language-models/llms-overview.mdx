---
sidebarTitle: "Using Cloud & Local Models"
---

---

<img
  className="mx-auto"
/>

---

## Compatible AI Models with Pieces

Pieces utilizes cloud-hosted LLMs from providers like OpenAI, Anthropic, and Google. All local models are served through [Ollama, a core dependency of PiecesOS](https://docs.pieces.app/products/core-dependencies/ollama) and the rest of the Pieces Suite.

## Cloud-Only LLMs | Providers

[Browse the list of cloud-hosted AI models available for use](https://docs.pieces.app/products/large-language-models/cloud-models) with the Pieces Desktop App and several of our plugins and extensions.

---

| **Provider** | **Model Name**                 |
| ------------ | ------------------------------ |
| _OpenAI_     | GPT-X                          |
| _Anthropic_  | Claude / Sonnet / Opus / Haiku |
| _Google_     | Gemini / Pro / Flash / Chat    |

---

## Local-Only LLMs | Providers

Read [through the list of local AI models available for use](https://docs.pieces.app/products/large-language-models/local-models) within the Pieces Desktop App and the rest of the Pieces Suite.

---

| **Provider** | **Model Name**               |
| ------------ | ---------------------------- |
| _Google_     | Gemma / Code Gemma           |
| _IBM_        | Granite / Code / Dense / MoE |
| _Meta_       | LLaMA / CodeLLaMA            |
| _Mistral_    | Mistral / Mixtral            |
| _Microsoft_  | Phi                          |
| _Qwen_       | QwQ / Coder                  |
| _StarCoder_  | StarCoder                    |

---

### Using Ollama with Pieces

Ollama is _required_ to utilize local generative AI features.

Itâ€™s a lightweight system that allows for a plug-and-play experience with local models, meaning that Pieces can update the number of compatible models we serve at lightning-speeds\!

If you want to read more about installing and using Ollama with Pieces, [click here for the Ollama section of our Core Dependencies documentation.](https://docs.pieces.app/products/core-dependencies/ollama)
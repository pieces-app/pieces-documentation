---
sidebarTitle: "LLM Settings"
---

---

## Switching LLMs

The <a target="_blank" href="https://marketplace.visualstudio.com/items?itemName=MeshIntelligentTechnologiesInc.pieces-vscode">
Pieces for VS Code Extension</a>

 currently supports [54 different LLMs](https://docs.pieces.app/products/large-language-models), including both cloud-hosted and local models.

## How To Configure Your LLM Runtime

Switching your LLM model in the Pieces for VS Code Extension is a straightforward process, giving you the flexibility to choose the model that best suits your needs.

How to change your LLM:

<Steps>
  <Step title="Open the Copilot Chat View">
    Open the Copilot Chat view by clicking the **Pieces Copilot** icon in the sidebar.
  </Step>
  <Step title="Locate the Active Model">
    Locate the **Active Model** in the bottom-left corner of the view where the current model (e.g., _GPT-4o Mini_) is displayed.

    <img
      className="mx-auto"
    />
  </Step>
  <Step title="View the Models">
    Click on `Change Model` to open the **Manage Copilot Runtime** modal.
  </Step>
  <Step title="Choose Your Desired Model">
    Browse the list of local and cloud models and select your preferred model.
  </Step>
</Steps>

From here, you can browse and select from a variety of available models, such as the local and cloud-based models listed [in the tables on this page.](https://docs.pieces.app/products/core-dependencies/ollama/supported-models)

<tip>
  Cloud-hosted models offer access to the latest AI capabilities, while on-device models ensure offline functionality, making Pieces Copilot adaptable to your specific workflow and environment.
</tip>

Once you’ve chosen a new model, the switch is instant, allowing you to continue your work seamlessly with the selected model's capabilities—_no need to restart or refresh anything._

<img
  className="mx-auto"
/>
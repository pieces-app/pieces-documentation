# Configuration

Read the guide below to set up the Pieces Web Extension to fit your workflow and preferences.

## Supported LLMs

We regularly update and configure our plugins and extensions to work with the latest LLMs.

The Pieces Web Extension currently supports a range of cloud-hosted and local LLMs from your favorite providers, such as Anthropic, Google, OpenAI, Microsoft, and more.

Read documentation on switching your Pieces Copilot Runtime (LLM) utilized by the Pieces Web Extension.

## Opening Pieces Settings

To open the settings within the Pieces Web Extension, follow these steps:

From your browser’s extensions list, select the Pieces icon.

In the top-right of the extension, select the sandwich icon.

After opening the horizontal bar menu, select the Settings button. This will open the available settings for the Pieces Web Extension.



## Settings Overview

Below is a breakdown of each adjustable setting (preferences, behaviors, views, etc.) available in the Pieces Web Extension, updated according to the latest features.

Displays your Pieces account details and cloud‑integration status.

Profile & Early Access: Your name, email, GitHub link, and Early Access enrollment

Personal Cloud: Connection status, last sync time, and custom domain (e.g., nolanworksat.pieces.cloud)

Trigger a manual backup of your local data to the cloud or restore from a snapshot.

Choose which language model powers Copilot in the extension (e.g. GPT‑4o Mini, GPT‑4o).

You can switch between local and cloud models here using the cloud and desktop icons.

Select the UI accent color used throughout the extension (Blue, Green, Purple, etc.).

When checked, your saved long‑term memory is automatically included in every Pieces Copilot query without needing to re-enable LTM chat-by-chat.

Clears all saved Pieces Copilot chat history inside the Pieces Web Extension.

This action is not reversible. Clearing your chat history is a permanent action.

Choose how much automatic enrichment occurs when saving materials. This can be beneficial for speed and performance on older systems, or if you’re utilizing local-only LLMs.

None: No metadata

Low: Basic metadata

Medium: Intermediate metadata (3–6 data points)

High: Extensive metadata (6+ data points)

Choose how Pieces utilizes cloud and local LLMs to process context and power other Pieces features.

Processing Mode: Cloud‑based / Local (on‑device) / Blended

Long‑Term Memory Engine: On/off toggle and engine version (e.g., LTM‑2)

When generating context, choose which sources (chats, visited pages, etc.) the LTM-2 engine draws context from.

This can also be done within the PiecesOS Quick Menu in the Long-Term Memory Engine Access Control panel.

Delete persisted memory captured by the LTM engine for a specified time range.

Automatically shows the floating Pieces button on every webpage, so your extension is always running.

Toggle the Pieces toolbar on or off for just the current domain.

Position the floating action button (Bottom Left, Bottom Right, etc.).

Use the action button's light or dark theme to match your browser.

Set the minimum character count before the extension recognizes a code block. This is helpful for blacklisting short, one-line code elements, like terminal commands.

Configure which actions (copy, save snippet, run tests, etc.) appear on detected code blocks.

Limit how many domains Pieces will track for snippet‑saving history.

If checked, newly visited domains are automatically added to your “recent websites” list.

Include the page’s metadata and content whenever you save a snippet. These websites can then be referenced and added as context to future Pieces Copilot chats.

Opt in or out of sharing anonymous usage metrics and crash reports.

Quick link to the official extension documentation.

Open the feedback form or GitHub issues page for bugs and feature requests.

Shows the version of your local PiecesOS service (e.g. 11.3.1).

Manually check for updates to your local PiecesOS installation.

Displays the port (default: 39300) on which your local PiecesOS server is actively listening & posting requests to.

For additional support resources, check out our troubleshooting guide.